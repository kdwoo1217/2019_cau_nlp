{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HHJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HHJ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import nmf\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "#for NMF model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\n",
    "from sklearn.decomposition import NMF;\n",
    "from sklearn.preprocessing import normalize;\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])  # 불용어처리\n",
    "\n",
    "#  빈도수 높은 키워드 처리\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use',' court', 'home',' council', 'hunter', 'help', 'time', 'injure', 'national', 'build', 'end', 'bid', 'cup', 'un', 'come', 'security', 'volunteer', 'ship', 'crew', 'crowd', 'join', 'helicopter', 'across', 'museum', 'Italy', 'grind', 'asian', 'sa', 'miss', 'one', 'die', 'use', 'three', 'Darwin', 'vic', 'number', 'may', 'start', 'law', 'way', 'communities', 'order', 'check', 'major', 'india', 'focus', 'form', 'journalist', 'milk', 'nz', 'rank', 'cook', 'egypt', 'New', 'year', 'force', 'fail', 'dead', 'was', 'farmer', 'fruit', 'philippines', 'injury', 'nick'])\n",
    "stop_words.extend(['fire', 'new', 'hobart', 'rural', 'world', 'boat', 'turn', 'flight', 'around', 'well', 'Find', 'two', 'adelaide', 'murder', 'first', 'make', 'body', 'probe', 'outback', 'tourism', 'baby', 'David', 'street', 'mass', 'hotel', 'Police', 'say', 'open', 'dog', 'go', 'welcome', 'president', 'announce', 'level', 'allow', 'highest','queensland', 'kill', 'crash', 'road', 'record', 'nt', 'hit', 'plane', 'toll', 'suspend', 'peninsula', 'afghan', 'recovery','man', 'perth', 'flood', 'people', 'prison', 'still', 'supply', 'siege', 'spark', 'summer', 'Michael', 'ops', 'large', 'flash', 'view', 'attack', 'back', 'mine', 'deal', 'fan', 'celebrate', 'target', 'hill', 'party', 'reveal', 'terrorism', 'video', 'pressure', 'remember', 'korea', 'indian', 'millions', 'drill', 'country', 'hour', 'podcast', 'leaders', 'thursday', 'abbott', 'tony', 'policy', 'agricultural', 'shorten', 'sach', 'day', 'years', 'show', 'teen', 'heat', 'sport', 'issue', 'free', 'australias', 'asbestos', 'compete','South', 'china', 'talk', 'appeal', 'labor', 'plant', 'peter', 'allegedly', 'begin', 'try', 'ice', 'native', 'alcohol', 'Australia', 'league', 'live', 'launch', 'campaign', 'benefit', 'update', 'stream', 'cabinet', 'document', 'bob','Test', 'drug', 'brisbane', 'international', 'british', 'double', 'treat', 'patient', 'ebola', 'Wa', 'bushfire', 'research', 'expansion', 'ready', 'old', 'release', 'paper', 'see'])\n",
    "\n",
    "# 그 아래에서  빈도수 높은 키워드 처리\n",
    "stop_words.extend(['call',  'queensland', 'melbourne', 'perth', 'thousands', 'alert', 'reveal', 'spark', 'amid', 'illegal', 'australian', 'price',  'brisbane', 'western', 'high', 'fan', 'prepare', 'british', 'battle', 'beach', 'wa', 'take',  'box', 'could',  'search', 'black', 'michael', 'week','man', 'day' ,'country', 'new', 'old', 'police', 'test',  'force', 'release', 'hobart', 'council', 'die', 'miss','say', 'south', 'was','fire', 'victoria', 'build','australia', 'court','find', 'fall','mine','attack', 'darwin', 'break', 'record', 'david', 'reflect', 'remember','adelaide', 'show'])\n",
    "stop_words.extend(['set','cut','put','us','january','febrary','april','may','june','july', 'august', 'november', 'september', 'october', 'december', 'july', 'march', 'wednesday', 'february','monday','tuesday','wednesday','thursday','friday','sunday','get','act','sydney','iraq'])\n",
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('C:\\\\Users\\\\HHJ\\\\Downloads\\\\NMF\\\\abcnews-date-text.csv', error_bad_lines=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['publish_date'] = pd.to_datetime(data['publish_date'].astype(str), format = '%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['publish_date'] = pd.DatetimeIndex(data['publish_date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data['headline_text']\n",
    "#data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data['headline_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_words_nostops = pd.Series(data_words_nostops)\n",
    "data_lemmatized = data_words_nostops.apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized_and_removed = remove_stopwords(data_lemmatized)\n",
    "temp = data\n",
    "temp['lemmatize'] = data_lemmatized_and_removed\n",
    "for i in range(15):\n",
    "    globals()['trend{}'.format(i+2003)] = temp.loc[temp.publish_date == i+2003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_list = [trend2003, trend2004, trend2005, trend2006, trend2007, trend2008, trend2009, trend2010, trend2011, trend2012, trend2013, trend2014, trend2015, trend2016, trend2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trend2003  # 2003년의 트렌드를 보기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = list(trend2005['lemmatize'])  # 2015년 트렌드\n",
    "id2word = corpora.Dictionary(lemmatized)\n",
    "texts = lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10;\n",
    " \n",
    "sentence = [' '.join(text) for text in lemmatized ]\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word', max_features=5000);\n",
    "x_counts = vectorizer.fit_transform(sentence);\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False);\n",
    "x_tfidf = transformer.fit_transform(x_counts);\n",
    "\n",
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain a NMF model.\n",
    "nmf_model = NMF(n_components=num_topics, init='nndsvd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0,\n",
       "  max_iter=200, n_components=10, random_state=None, shuffle=False,\n",
       "  solver='cd', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model\n",
    "nmf_model.fit(xtfidf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;       \n",
    "    return pd.DataFrame(word_dict);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "      <th>Topic # 06</th>\n",
       "      <th>Topic # 07</th>\n",
       "      <th>Topic # 08</th>\n",
       "      <th>Topic # 09</th>\n",
       "      <th>Topic # 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>charge</td>\n",
       "      <td>plan</td>\n",
       "      <td>govt</td>\n",
       "      <td>car</td>\n",
       "      <td>continue</td>\n",
       "      <td>death</td>\n",
       "      <td>win</td>\n",
       "      <td>face</td>\n",
       "      <td>hospital</td>\n",
       "      <td>urge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>assault</td>\n",
       "      <td>water</td>\n",
       "      <td>accuse</td>\n",
       "      <td>bomb</td>\n",
       "      <td>investigation</td>\n",
       "      <td>investigate</td>\n",
       "      <td>claim</td>\n",
       "      <td>accuse</td>\n",
       "      <td>service</td>\n",
       "      <td>seek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stab</td>\n",
       "      <td>centre</td>\n",
       "      <td>claim</td>\n",
       "      <td>accident</td>\n",
       "      <td>woman</td>\n",
       "      <td>jail</td>\n",
       "      <td>award</td>\n",
       "      <td>jail</td>\n",
       "      <td>woman</td>\n",
       "      <td>fund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>woman</td>\n",
       "      <td>consider</td>\n",
       "      <td>urge</td>\n",
       "      <td>woman</td>\n",
       "      <td>blaze</td>\n",
       "      <td>rise</td>\n",
       "      <td>title</td>\n",
       "      <td>ban</td>\n",
       "      <td>doctor</td>\n",
       "      <td>warn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>shoot</td>\n",
       "      <td>house</td>\n",
       "      <td>consider</td>\n",
       "      <td>fatal</td>\n",
       "      <td>house</td>\n",
       "      <td>shoot</td>\n",
       "      <td>election</td>\n",
       "      <td>trial</td>\n",
       "      <td>leave</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>raid</td>\n",
       "      <td>opposition</td>\n",
       "      <td>reject</td>\n",
       "      <td>baghdad</td>\n",
       "      <td>storm</td>\n",
       "      <td>arrest</td>\n",
       "      <td>lead</td>\n",
       "      <td>assault</td>\n",
       "      <td>report</td>\n",
       "      <td>boost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lay</td>\n",
       "      <td>change</td>\n",
       "      <td>defend</td>\n",
       "      <td>victim</td>\n",
       "      <td>rise</td>\n",
       "      <td>tsunami</td>\n",
       "      <td>top</td>\n",
       "      <td>sex</td>\n",
       "      <td>staff</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sex</td>\n",
       "      <td>development</td>\n",
       "      <td>offer</td>\n",
       "      <td>name</td>\n",
       "      <td>push</td>\n",
       "      <td>investigation</td>\n",
       "      <td>final</td>\n",
       "      <td>hear</td>\n",
       "      <td>defend</td>\n",
       "      <td>change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>drop</td>\n",
       "      <td>concern</td>\n",
       "      <td>fund</td>\n",
       "      <td>park</td>\n",
       "      <td>tourist</td>\n",
       "      <td>suspicious</td>\n",
       "      <td>play</td>\n",
       "      <td>water</td>\n",
       "      <td>pope</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>guilty</td>\n",
       "      <td>reject</td>\n",
       "      <td>local</td>\n",
       "      <td>house</td>\n",
       "      <td>firefighters</td>\n",
       "      <td>question</td>\n",
       "      <td>race</td>\n",
       "      <td>stab</td>\n",
       "      <td>inquiry</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rape</td>\n",
       "      <td>management</td>\n",
       "      <td>rule</td>\n",
       "      <td>chase</td>\n",
       "      <td>fight</td>\n",
       "      <td>sentence</td>\n",
       "      <td>stage</td>\n",
       "      <td>pair</td>\n",
       "      <td>health</td>\n",
       "      <td>water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>arrest</td>\n",
       "      <td>group</td>\n",
       "      <td>tax</td>\n",
       "      <td>arrest</td>\n",
       "      <td>fatal</td>\n",
       "      <td>stab</td>\n",
       "      <td>return</td>\n",
       "      <td>rape</td>\n",
       "      <td>stab</td>\n",
       "      <td>minister</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>child</td>\n",
       "      <td>work</td>\n",
       "      <td>ask</td>\n",
       "      <td>blast</td>\n",
       "      <td>clean</td>\n",
       "      <td>inquiry</td>\n",
       "      <td>third</td>\n",
       "      <td>woman</td>\n",
       "      <td>minister</td>\n",
       "      <td>report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>front</td>\n",
       "      <td>airport</td>\n",
       "      <td>nsw</td>\n",
       "      <td>four</td>\n",
       "      <td>drought</td>\n",
       "      <td>suspect</td>\n",
       "      <td>tour</td>\n",
       "      <td>driver</td>\n",
       "      <td>bundaberg</td>\n",
       "      <td>house</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>drive</td>\n",
       "      <td>power</td>\n",
       "      <td>delay</td>\n",
       "      <td>boy</td>\n",
       "      <td>market</td>\n",
       "      <td>claim</td>\n",
       "      <td>poll</td>\n",
       "      <td>bali</td>\n",
       "      <td>claim</td>\n",
       "      <td>tsunami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>plead</td>\n",
       "      <td>community</td>\n",
       "      <td>criticise</td>\n",
       "      <td>london</td>\n",
       "      <td>run</td>\n",
       "      <td>inquest</td>\n",
       "      <td>prize</td>\n",
       "      <td>child</td>\n",
       "      <td>site</td>\n",
       "      <td>mp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>trio</td>\n",
       "      <td>question</td>\n",
       "      <td>indigenous</td>\n",
       "      <td>highway</td>\n",
       "      <td>burn</td>\n",
       "      <td>blast</td>\n",
       "      <td>tigers</td>\n",
       "      <td>final</td>\n",
       "      <td>patients</td>\n",
       "      <td>farmers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>attempt</td>\n",
       "      <td>residents</td>\n",
       "      <td>stand</td>\n",
       "      <td>hurt</td>\n",
       "      <td>tas</td>\n",
       "      <td>rate</td>\n",
       "      <td>blue</td>\n",
       "      <td>deportation</td>\n",
       "      <td>blaze</td>\n",
       "      <td>ir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>four</td>\n",
       "      <td>meet</td>\n",
       "      <td>deny</td>\n",
       "      <td>investigate</td>\n",
       "      <td>investigations</td>\n",
       "      <td>prompt</td>\n",
       "      <td>england</td>\n",
       "      <td>challenge</td>\n",
       "      <td>head</td>\n",
       "      <td>fight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>stand</td>\n",
       "      <td>park</td>\n",
       "      <td>ir</td>\n",
       "      <td>suspect</td>\n",
       "      <td>violence</td>\n",
       "      <td>custody</td>\n",
       "      <td>post</td>\n",
       "      <td>future</td>\n",
       "      <td>bed</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic # 01   Topic # 02  Topic # 03   Topic # 04      Topic # 05  \\\n",
       "0      charge         plan        govt          car        continue   \n",
       "1     assault        water      accuse         bomb   investigation   \n",
       "2        stab       centre       claim     accident           woman   \n",
       "3       woman     consider        urge        woman           blaze   \n",
       "4       shoot        house    consider        fatal           house   \n",
       "5        raid   opposition      reject      baghdad           storm   \n",
       "6         lay       change      defend       victim            rise   \n",
       "7         sex  development       offer         name            push   \n",
       "8        drop      concern        fund         park         tourist   \n",
       "9      guilty       reject       local        house    firefighters   \n",
       "10       rape   management        rule        chase           fight   \n",
       "11     arrest        group         tax       arrest           fatal   \n",
       "12      child         work         ask        blast           clean   \n",
       "13      front      airport         nsw         four         drought   \n",
       "14      drive        power       delay          boy          market   \n",
       "15      plead    community   criticise       london             run   \n",
       "16       trio     question  indigenous      highway            burn   \n",
       "17    attempt    residents       stand         hurt             tas   \n",
       "18       four         meet        deny  investigate  investigations   \n",
       "19      stand         park          ir      suspect        violence   \n",
       "\n",
       "       Topic # 06 Topic # 07   Topic # 08 Topic # 09 Topic # 10  \n",
       "0           death        win         face   hospital       urge  \n",
       "1     investigate      claim       accuse    service       seek  \n",
       "2            jail      award         jail      woman       fund  \n",
       "3            rise      title          ban     doctor       warn  \n",
       "4           shoot   election        trial      leave     public  \n",
       "5          arrest       lead      assault     report      boost  \n",
       "6         tsunami        top          sex      staff     health  \n",
       "7   investigation      final         hear     defend     change  \n",
       "8      suspicious       play        water       pope    service  \n",
       "9        question       race         stab    inquiry      group  \n",
       "10       sentence      stage         pair     health      water  \n",
       "11           stab     return         rape       stab   minister  \n",
       "12        inquiry      third        woman   minister     report  \n",
       "13        suspect       tour       driver  bundaberg      house  \n",
       "14          claim       poll         bali      claim    tsunami  \n",
       "15        inquest      prize        child       site         mp  \n",
       "16          blast     tigers        final   patients    farmers  \n",
       "17           rate       blue  deportation      blaze         ir  \n",
       "18         prompt    england    challenge       head      fight  \n",
       "19        custody       post       future        bed       work  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get_nmf_topics(nmf_model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = gensim.models.nmf.Nmf(corpus,\n",
    "                                   num_topics,\n",
    "                                   id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.080*\"group\" + 0.048*\"health\" + 0.047*\"warn\" + 0.034*\"service\" + '\n",
      "  '0.025*\"boost\" + 0.011*\"aid\" + 0.010*\"claim\" + 0.009*\"mental\" + 0.008*\"nsw\" '\n",
      "  '+ 0.007*\"indigenous\"'),\n",
      " (1,\n",
      "  '0.215*\"urge\" + 0.026*\"fund\" + 0.022*\"win\" + 0.016*\"public\" + 0.009*\"boost\" '\n",
      "  '+ 0.008*\"rethink\" + 0.007*\"action\" + 0.005*\"mp\" + 0.005*\"drivers\" + '\n",
      "  '0.004*\"change\"'),\n",
      " (2,\n",
      "  '0.240*\"plan\" + 0.018*\"aid\" + 0.007*\"change\" + 0.006*\"coast\" + 0.006*\"fear\" '\n",
      "  '+ 0.006*\"centre\" + 0.005*\"ir\" + 0.004*\"public\" + 0.004*\"house\" + '\n",
      "  '0.004*\"community\"'),\n",
      " (3,\n",
      "  '0.054*\"water\" + 0.023*\"offer\" + 0.018*\"return\" + 0.015*\"ban\" + 0.013*\"lead\" '\n",
      "  '+ 0.011*\"qld\" + 0.010*\"consider\" + 0.009*\"rise\" + 0.009*\"boost\" + '\n",
      "  '0.008*\"concern\"'),\n",
      " (4,\n",
      "  '0.128*\"death\" + 0.038*\"hospital\" + 0.023*\"rise\" + 0.014*\"investigate\" + '\n",
      "  '0.013*\"aceh\" + 0.013*\"school\" + 0.012*\"report\" + 0.007*\"fear\" + '\n",
      "  '0.006*\"shoot\" + 0.006*\"inquest\"'),\n",
      " (5,\n",
      "  '0.115*\"charge\" + 0.061*\"claim\" + 0.019*\"face\" + 0.014*\"sex\" + '\n",
      "  '0.014*\"reject\" + 0.013*\"change\" + 0.012*\"child\" + 0.011*\"assault\" + '\n",
      "  '0.009*\"work\" + 0.008*\"centre\"'),\n",
      " (6,\n",
      "  '0.076*\"seek\" + 0.025*\"win\" + 0.016*\"report\" + 0.016*\"car\" + 0.012*\"power\" + '\n",
      "  '0.011*\"opposition\" + 0.010*\"change\" + 0.009*\"house\" + 0.008*\"public\" + '\n",
      "  '0.008*\"warn\"'),\n",
      " (7,\n",
      "  '0.185*\"tsunami\" + 0.025*\"relief\" + 0.024*\"warn\" + 0.015*\"service\" + '\n",
      "  '0.014*\"aid\" + 0.014*\"continue\" + 0.013*\"indonesia\" + 0.011*\"match\" + '\n",
      "  '0.009*\"sri\" + 0.008*\"system\"'),\n",
      " (8,\n",
      "  '0.099*\"face\" + 0.066*\"fund\" + 0.047*\"victims\" + 0.025*\"hospital\" + '\n",
      "  '0.013*\"aid\" + 0.013*\"aust\" + 0.011*\"raise\" + 0.011*\"tsunami\" + 0.008*\"car\" '\n",
      "  '+ 0.007*\"accuse\"'),\n",
      " (9,\n",
      "  '0.268*\"govt\" + 0.022*\"fund\" + 0.011*\"seek\" + 0.009*\"accuse\" + '\n",
      "  '0.008*\"defend\" + 0.008*\"local\" + 0.007*\"ask\" + 0.007*\"consider\" + '\n",
      "  '0.005*\"indigenous\" + 0.005*\"ir\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(nmf_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
