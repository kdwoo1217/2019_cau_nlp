{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import scipy as sp;\n",
    "import sys;\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords;\n",
    "import nltk;\n",
    "\n",
    "from gensim.models import ldamodel\n",
    "#from gensim.models.nmf import NMF\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.corpora;\n",
    "\n",
    "import sklearn;\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\n",
    "from sklearn.decomposition import NMF;\n",
    "from sklearn.preprocessing import normalize;\n",
    "\n",
    "import pickle;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/kdwoo/Documents/Jupyter/2019_CAU_NLP/abcnews-date-text.csv', error_bad_lines=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['publish_date'] = pd.to_datetime(data['publish_date'].astype(str), format = '%Y%m%d')\n",
    "data['publish_date'] = pd.DatetimeIndex(data['publish_date']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data[['headline_text']];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data['headline_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_nostops = pd.Series(data_words_nostops)\n",
    "data_lemmatized = data_words_nostops.apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [aba, decides, community, broadcasting, licence]\n",
       "1            [act, fire, witnesses, must, aware, defamation]\n",
       "2                [calls, infrastructure, protection, summit]\n",
       "3                  [air, nz, staff, aust, strike, pay, rise]\n",
       "4          [air, nz, strike, affect, australian, travellers]\n",
       "5                    [ambitious, olsson, wins, triple, jump]\n",
       "6                [antic, delighted, record, breaking, barca]\n",
       "7          [aussie, qualifier, stosur, wastes, four, memp...\n",
       "8             [aust, addresses, un, security, council, iraq]\n",
       "9                   [australia, locked, war, timetable, opp]\n",
       "10               [australia, contribute, million, aid, iraq]\n",
       "11         [barca, take, record, robson, celebrates, birt...\n",
       "12                           [bathhouse, plans, move, ahead]\n",
       "13           [big, hopes, launceston, cycling, championship]\n",
       "14                [big, plan, boost, paroo, water, supplies]\n",
       "15                 [blizzard, buries, united, states, bills]\n",
       "16         [brigadier, dismisses, reports, troops, harassed]\n",
       "17         [british, combat, troops, arriving, daily, kuw...\n",
       "18            [bryant, leads, lakers, double, overtime, win]\n",
       "19               [bushfire, victims, urged, see, centrelink]\n",
       "20                 [businesses, prepare, terrorist, attacks]\n",
       "21         [calleri, avenges, final, defeat, eliminate, m...\n",
       "22                   [call, ethanol, blend, fuel, go, ahead]\n",
       "23                [carews, freak, goal, leaves, roma, ruins]\n",
       "24                                 [cemeteries, miss, funds]\n",
       "25         [code, conduct, toughens, organ, donation, reg...\n",
       "26         [commonwealth, bank, cuts, fixed, home, loan, ...\n",
       "27                 [community, urged, help, homeless, youth]\n",
       "28         [council, chief, executive, fails, secure, pos...\n",
       "29            [councillor, contest, wollongong, independent]\n",
       "                                 ...                        \n",
       "1103633        [nepal, bans, solo, climbers, mount, everest]\n",
       "1103634     [new, years, eve, celebrated, around, australia]\n",
       "1103635        [new, years, eve, australia, prepares, bring]\n",
       "1103636       [new, years, eve, celebrations, around, world]\n",
       "1103637    [new, years, texting, data, load, surge, clock...\n",
       "1103638    [north, korea, leader, kim, jong, un, watches,...\n",
       "1103639          [real, tourists, converge, sydney, harbour]\n",
       "1103640    [nye, guide, sydney, best, venues, public, tra...\n",
       "1103641     [police, confirm, deaths, six, people, seaplane]\n",
       "1103642    [police, officer, brett, forte, killed, shooti...\n",
       "1103643           [plate, driver, caught, kph, speed, limit]\n",
       "1103644           [protesters, throw, rocks, police, tehran]\n",
       "1103645               [remembering, australian, lives, lost]\n",
       "1103646     [remount, horsemanship, helping, veterans, ptsd]\n",
       "1103647    [roger, federer, rivals, battling, injury, ahe...\n",
       "1103648    [russian, tankers, fuelled, north, korea, via,...\n",
       "1103649    [sa, transport, department, defends, major, in...\n",
       "1103650        [sea, plane, crashed, hawkesbury, river, nsw]\n",
       "1103651    [search, survivors, hawkesbury, sea, plane, cr...\n",
       "1103652    [second, sexual, assault, reported, falls, fes...\n",
       "1103653    [severe, storms, forecast, nye, south, east, q...\n",
       "1103654     [snake, catcher, pleads, people, kill, reptiles]\n",
       "1103655    [south, australia, prepares, party, welcome, n...\n",
       "1103656           [strikers, cool, heat, big, win, adelaide]\n",
       "1103657            [stunning, images, sydney, hobart, yacht]\n",
       "1103658    [ashes, smiths, warners, near, miss, liven, bo...\n",
       "1103659         [timelapse, brisbanes, new, year, fireworks]\n",
       "1103660                             [meant, kids, australia]\n",
       "1103661            [papodopoulos, meeting, may, mean, ausus]\n",
       "1103662    [george, papadopoulos, former, trump, campaign...\n",
       "Length: 1103663, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words_nostops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])  # 불용어처리\n",
    "\n",
    "#  빈도수 높은 키워드 처리\n",
    "stop_words.extend([' court', 'home',' council', 'hunter', 'help', 'time', 'injure', 'national', 'build', 'end', 'bid', 'cup', 'un', 'come', 'security', 'volunteer', 'ship', 'crew', 'crowd', 'join', 'helicopter', 'across', 'museum', 'Italy', 'grind', 'asian', 'sa', 'miss', 'one', 'die', 'use', 'three', 'Darwin', 'vic', 'number', 'may', 'start', 'law', 'way', 'communities', 'order', 'check', 'major', 'india', 'focus', 'form', 'journalist', 'milk', 'nz', 'rank', 'cook', 'egypt', 'New', 'year', 'force', 'fail', 'dead', 'was', 'farmer', 'fruit', 'philippines', 'injury', 'nick'])\n",
    "stop_words.extend(['fire', 'new', 'hobart', 'rural', 'world', 'boat', 'turn', 'flight', 'around', 'well', 'Find', 'two', 'adelaide', 'murder', 'first', 'make', 'body', 'probe', 'outback', 'tourism', 'baby', 'David', 'street', 'mass', 'hotel', 'Police', 'say', 'open', 'dog', 'go', 'welcome', 'president', 'announce', 'level', 'allow', 'highest','queensland', 'kill', 'crash', 'road', 'record', 'nt', 'hit', 'plane', 'toll', 'suspend', 'peninsula', 'afghan', 'recovery','man', 'perth', 'flood', 'people', 'prison', 'still', 'supply', 'siege', 'spark', 'summer', 'Michael', 'ops', 'large', 'flash', 'view', 'attack', 'back', 'mine', 'deal', 'fan', 'celebrate', 'target', 'hill', 'party', 'reveal', 'terrorism', 'video', 'pressure', 'remember', 'korea', 'indian', 'millions', 'drill', 'country', 'hour', 'podcast', 'leaders', 'thursday', 'abbott', 'tony', 'policy', 'agricultural', 'shorten', 'sach', 'day', 'years', 'show', 'teen', 'heat', 'sport', 'issue', 'free', 'australias', 'asbestos', 'compete','South', 'china', 'talk', 'appeal', 'labor', 'plant', 'peter', 'allegedly', 'begin', 'try', 'ice', 'native', 'alcohol', 'Australia', 'league', 'live', 'launch', 'campaign', 'benefit', 'update', 'stream', 'cabinet', 'document', 'bob','Test', 'drug', 'brisbane', 'international', 'british', 'double', 'treat', 'patient', 'ebola', 'Wa', 'bushfire', 'research', 'expansion', 'ready', 'old', 'release', 'paper', 'see'])\n",
    "\n",
    "# 그 아래에서  빈도수 높은 키워드 처리\n",
    "stop_words.extend(['call',  'queensland', 'melbourne', 'perth', 'thousands', 'alert', 'reveal', 'spark', 'amid', 'illegal', 'australian', 'price',  'brisbane', 'western', 'high', 'fan', 'prepare', 'british', 'battle', 'beach', 'wa', 'take',  'box', 'could',  'search', 'black', 'michael', 'week','man', 'day' ,'country', 'new', 'old', 'police', 'test',  'force', 'release', 'hobart', 'council', 'die', 'miss','say', 'south', 'was','fire', 'victoria', 'build','australia', 'court','find', 'fall','mine','attack', 'darwin', 'break', 'record', 'david', 'reflect', 'remember','adelaide', 'show'])\n",
    "\n",
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = remove_stopwords(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data\n",
    "temp['lemmatize'] = data_lemmatized\n",
    "for i in range(15):\n",
    "    globals()['trend{}'.format(i+2003)] = temp.loc[temp.publish_date == i+2003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_list = [trend2003, trend2004, trend2005, trend2006, trend2007, trend2008, trend2009, trend2010,\n",
    "              trend2011, trend2012, trend2013, trend2014, trend2015, trend2016, trend2017]\n",
    "#print(trend_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(data_text, open('2003_data_text.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_headlines = [value[0] for value in data_text.iloc[0:].values];\n",
    "#print(train_headlines[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10;\n",
    "lemmatized = list(trend2017['lemmatize'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_headlines_sentences = [' '.join(text) for text in train_headlines]\n",
    "#print(train_headlines_sentences[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer='word', max_features=5000);\n",
    "x_counts = vectorizer.fit_transform(' '.join(text) for text in list(trend2003['lemmatize']));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer(smooth_idf=False);\n",
    "x_tfidf = transformer.fit_transform(x_counts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain a NMF model.\n",
    "model = NMF(n_components=num_topics, init='nndsvd');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init='nndsvd', l1_ratio=0.0,\n",
       "  max_iter=200, n_components=10, random_state=None, shuffle=False,\n",
       "  solver='cd', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model\n",
    "model.fit(xtfidf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4999, 4998, 4997, 4996, 4995, 4994, 4993, 4992, 4991, 4990, 4989,\n",
       "       4988, 4987, 4986, 4985, 4984, 4983, 4982, 4981, 4980], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.components_[0].argsort()[:-20 - 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.37014285, 0.22685139, 0.19492771, 0.12986697, 0.09437241,\n",
       "       0.0908397 , 0.07772541, 0.06861983, 0.06785875, 0.06509755,\n",
       "       0.06496493, 0.06108526, 0.06028505, 0.05443715, 0.05275734,\n",
       "       0.05229953, 0.05213447, 0.04990521, 0.04848382, 0.04642985])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.components_[0][:-20 - 1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nmf_topics(model, n_top_words):\n",
    "    \n",
    "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    vocabulary = vectorizer.vocabulary_\n",
    "    \n",
    "    word_dict = {};\n",
    "    for i in range(num_topics):\n",
    "        \n",
    "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
    "        words_ids = model.components_[i].argsort()[:-20 - 1:-1]\n",
    "        words = [feat_names[key] for key in words_ids]\n",
    "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
    "    \n",
    "    return pd.DataFrame(word_dict);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic # 01</th>\n",
       "      <th>Topic # 02</th>\n",
       "      <th>Topic # 03</th>\n",
       "      <th>Topic # 04</th>\n",
       "      <th>Topic # 05</th>\n",
       "      <th>Topic # 06</th>\n",
       "      <th>Topic # 07</th>\n",
       "      <th>Topic # 08</th>\n",
       "      <th>Topic # 09</th>\n",
       "      <th>Topic # 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>charge</td>\n",
       "      <td>us</td>\n",
       "      <td>plan</td>\n",
       "      <td>govt</td>\n",
       "      <td>face</td>\n",
       "      <td>win</td>\n",
       "      <td>continue</td>\n",
       "      <td>iraq</td>\n",
       "      <td>car</td>\n",
       "      <td>fund</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stab</td>\n",
       "      <td>soldier</td>\n",
       "      <td>development</td>\n",
       "      <td>nsw</td>\n",
       "      <td>trial</td>\n",
       "      <td>award</td>\n",
       "      <td>death</td>\n",
       "      <td>war</td>\n",
       "      <td>woman</td>\n",
       "      <td>boost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>assault</td>\n",
       "      <td>iraqi</td>\n",
       "      <td>reject</td>\n",
       "      <td>urge</td>\n",
       "      <td>death</td>\n",
       "      <td>claim</td>\n",
       "      <td>rise</td>\n",
       "      <td>soldier</td>\n",
       "      <td>accident</td>\n",
       "      <td>seek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>attempt</td>\n",
       "      <td>troop</td>\n",
       "      <td>consider</td>\n",
       "      <td>qld</td>\n",
       "      <td>pair</td>\n",
       "      <td>top</td>\n",
       "      <td>sydney</td>\n",
       "      <td>troop</td>\n",
       "      <td>hospital</td>\n",
       "      <td>water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sex</td>\n",
       "      <td>baghdad</td>\n",
       "      <td>house</td>\n",
       "      <td>claim</td>\n",
       "      <td>tough</td>\n",
       "      <td>lead</td>\n",
       "      <td>investigation</td>\n",
       "      <td>bush</td>\n",
       "      <td>fatal</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lay</td>\n",
       "      <td>shoot</td>\n",
       "      <td>group</td>\n",
       "      <td>consider</td>\n",
       "      <td>accuse</td>\n",
       "      <td>title</td>\n",
       "      <td>fight</td>\n",
       "      <td>report</td>\n",
       "      <td>shoot</td>\n",
       "      <td>warn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sydney</td>\n",
       "      <td>military</td>\n",
       "      <td>water</td>\n",
       "      <td>reject</td>\n",
       "      <td>ban</td>\n",
       "      <td>tour</td>\n",
       "      <td>protest</td>\n",
       "      <td>howard</td>\n",
       "      <td>investigate</td>\n",
       "      <td>health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fraud</td>\n",
       "      <td>trade</td>\n",
       "      <td>management</td>\n",
       "      <td>feed</td>\n",
       "      <td>water</td>\n",
       "      <td>england</td>\n",
       "      <td>strike</td>\n",
       "      <td>pm</td>\n",
       "      <td>sydney</td>\n",
       "      <td>service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shoot</td>\n",
       "      <td>warn</td>\n",
       "      <td>protest</td>\n",
       "      <td>accuse</td>\n",
       "      <td>future</td>\n",
       "      <td>set</td>\n",
       "      <td>house</td>\n",
       "      <td>bomb</td>\n",
       "      <td>house</td>\n",
       "      <td>concern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bail</td>\n",
       "      <td>bomb</td>\n",
       "      <td>power</td>\n",
       "      <td>local</td>\n",
       "      <td>restrictions</td>\n",
       "      <td>stage</td>\n",
       "      <td>clean</td>\n",
       "      <td>downer</td>\n",
       "      <td>stab</td>\n",
       "      <td>report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>front</td>\n",
       "      <td>iraqis</td>\n",
       "      <td>shire</td>\n",
       "      <td>tas</td>\n",
       "      <td>uncertain</td>\n",
       "      <td>race</td>\n",
       "      <td>debate</td>\n",
       "      <td>defend</td>\n",
       "      <td>death</td>\n",
       "      <td>urge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>drop</td>\n",
       "      <td>death</td>\n",
       "      <td>residents</td>\n",
       "      <td>act</td>\n",
       "      <td>jail</td>\n",
       "      <td>election</td>\n",
       "      <td>sars</td>\n",
       "      <td>blair</td>\n",
       "      <td>bomb</td>\n",
       "      <td>rain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rape</td>\n",
       "      <td>saddam</td>\n",
       "      <td>park</td>\n",
       "      <td>defend</td>\n",
       "      <td>allege</td>\n",
       "      <td>final</td>\n",
       "      <td>row</td>\n",
       "      <td>aid</td>\n",
       "      <td>claim</td>\n",
       "      <td>set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>woman</td>\n",
       "      <td>wound</td>\n",
       "      <td>change</td>\n",
       "      <td>review</td>\n",
       "      <td>sex</td>\n",
       "      <td>return</td>\n",
       "      <td>shoot</td>\n",
       "      <td>blast</td>\n",
       "      <td>arrest</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>offences</td>\n",
       "      <td>north</td>\n",
       "      <td>meet</td>\n",
       "      <td>move</td>\n",
       "      <td>rape</td>\n",
       "      <td>series</td>\n",
       "      <td>blaze</td>\n",
       "      <td>resolution</td>\n",
       "      <td>highway</td>\n",
       "      <td>air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>child</td>\n",
       "      <td>hold</td>\n",
       "      <td>merger</td>\n",
       "      <td>rule</td>\n",
       "      <td>challenge</td>\n",
       "      <td>gold</td>\n",
       "      <td>school</td>\n",
       "      <td>claim</td>\n",
       "      <td>driver</td>\n",
       "      <td>indigenous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>fatal</td>\n",
       "      <td>blast</td>\n",
       "      <td>move</td>\n",
       "      <td>inquiry</td>\n",
       "      <td>trio</td>\n",
       "      <td>hold</td>\n",
       "      <td>pay</td>\n",
       "      <td>protest</td>\n",
       "      <td>name</td>\n",
       "      <td>restrictions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>fight</td>\n",
       "      <td>market</td>\n",
       "      <td>support</td>\n",
       "      <td>report</td>\n",
       "      <td>final</td>\n",
       "      <td>put</td>\n",
       "      <td>investigations</td>\n",
       "      <td>leave</td>\n",
       "      <td>jail</td>\n",
       "      <td>drought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lead</td>\n",
       "      <td>claim</td>\n",
       "      <td>centre</td>\n",
       "      <td>opposition</td>\n",
       "      <td>sydney</td>\n",
       "      <td>streak</td>\n",
       "      <td>hunt</td>\n",
       "      <td>intelligence</td>\n",
       "      <td>collision</td>\n",
       "      <td>job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>robbery</td>\n",
       "      <td>grenade</td>\n",
       "      <td>green</td>\n",
       "      <td>deny</td>\n",
       "      <td>attempt</td>\n",
       "      <td>prize</td>\n",
       "      <td>storm</td>\n",
       "      <td>meet</td>\n",
       "      <td>leave</td>\n",
       "      <td>farmers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic # 01 Topic # 02   Topic # 03  Topic # 04    Topic # 05 Topic # 06  \\\n",
       "0      charge         us         plan        govt          face        win   \n",
       "1        stab    soldier  development         nsw         trial      award   \n",
       "2     assault      iraqi       reject        urge         death      claim   \n",
       "3     attempt      troop     consider         qld          pair        top   \n",
       "4         sex    baghdad        house       claim         tough       lead   \n",
       "5         lay      shoot        group    consider        accuse      title   \n",
       "6      sydney   military        water      reject           ban       tour   \n",
       "7       fraud      trade   management        feed         water    england   \n",
       "8       shoot       warn      protest      accuse        future        set   \n",
       "9        bail       bomb        power       local  restrictions      stage   \n",
       "10      front     iraqis        shire         tas     uncertain       race   \n",
       "11       drop      death    residents         act          jail   election   \n",
       "12       rape     saddam         park      defend        allege      final   \n",
       "13      woman      wound       change      review           sex     return   \n",
       "14   offences      north         meet        move          rape     series   \n",
       "15      child       hold       merger        rule     challenge       gold   \n",
       "16      fatal      blast         move     inquiry          trio       hold   \n",
       "17      fight     market      support      report         final        put   \n",
       "18       lead      claim       centre  opposition        sydney     streak   \n",
       "19    robbery    grenade        green        deny       attempt      prize   \n",
       "\n",
       "        Topic # 07    Topic # 08   Topic # 09    Topic # 10  \n",
       "0         continue          iraq          car          fund  \n",
       "1            death           war        woman         boost  \n",
       "2             rise       soldier     accident          seek  \n",
       "3           sydney         troop     hospital         water  \n",
       "4    investigation          bush        fatal           get  \n",
       "5            fight        report        shoot          warn  \n",
       "6          protest        howard  investigate        health  \n",
       "7           strike            pm       sydney       service  \n",
       "8            house          bomb        house       concern  \n",
       "9            clean        downer         stab        report  \n",
       "10          debate        defend        death          urge  \n",
       "11            sars         blair         bomb          rain  \n",
       "12             row           aid        claim           set  \n",
       "13           shoot         blast       arrest         group  \n",
       "14           blaze    resolution      highway           air  \n",
       "15          school         claim       driver    indigenous  \n",
       "16             pay       protest         name  restrictions  \n",
       "17  investigations         leave         jail       drought  \n",
       "18            hunt  intelligence    collision           job  \n",
       "19           storm          meet        leave       farmers  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nmf_topics(model, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
